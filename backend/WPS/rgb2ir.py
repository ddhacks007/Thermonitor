import itertools
import os
import glob
from tqdm.notebook import tqdm

import numpy as np
import torchvision.transforms as transforms
from torchvision.transforms import InterpolationMode
import torchvision
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch
from PIL import Image

from models import ResidualBlock, GeneratorResNet, Discriminator 
from utils import LambdaLR
from datasetsLoad import ImageDataset


def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution
        if hasattr(m, 'bias') and m.bias is not None:
            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)
        elif classname.find('BatchNorm2d') != -1:
            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution
            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)

  
   
def to_rgb(image):
    rgb_image = Image.new("RGB", image.size)
    rgb_image.paste(image)
    return rgb_image
  

def test(G_AB, 
        image_path, 
        transforms_ = None):
    G_AB.eval()
    transforms_ = transforms.Compose(transforms_)
    cuda = torch.cuda.is_available()
    Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor
    for i in glob.glob(image_path+'/*'):
        img = Image.open(i)
        img = transforms_(img)
        img = img.unsqueeze(0)
        img = img.type(Tensor)
        img_g = G_AB(img)
        torchvision.utils.save_image(img_g[0], "results/result"+os.path.basename(i).split(".")[0]+".png")
    G_AB.train()
    
def evaluate(G_AB,
        G_BA,
        D_A,
        D_B,
        val_loader, 
        criterion_GAN,
        criterion_cycle,
        criterion_identity):
    G_AB.eval()
    G_BA.eval()
    D_A.eval()
    D_B.eval()
    loss_D = 0.0
    loss_G = 0.0
    loss_GAN = 0.0
    loss_cycle = 0.0
    loss_identity = 0.0
    cuda = torch.cuda.is_available()
    Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor                  
    for i, batch in enumerate(val_loader):
            # Set model input
            real_A = batch['A'].type(Tensor)
            real_B = batch['B'].type(Tensor)
            
            # Adversarial ground truths
            valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.
            fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.        
  
            loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,
            loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.
                                                                 # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').
            loss_identity += (loss_id_A.item() + loss_id_B.item())/2
    #         loss_identity = loss_id_A
            
            # GAN Loss
            fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing
            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'
            fake_A = G_BA(real_B)
            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'
            
            loss_GAN += (loss_GAN_AB.item() + loss_GAN_BA.item())/2
            
            # Cycle Loss
            recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo
            loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image
            recov_B = G_AB(fake_A)
            loss_cycle_B = criterion_cycle(recov_B, real_B)
            
            loss_cycle += (loss_cycle_A.item() + loss_cycle_B.item())/2
            
    # ------> Total Loss
    
            # Discriminator A
            loss_G += loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)
            
            loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real
            loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake
            
            loss_D_A = (loss_real + loss_fake)/2
            
            # Discriminator B
            loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real
            loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake
            
            loss_D_B = (loss_real + loss_fake)/2
            
    # ------> Total Loss
            loss_D += (loss_D_A.item() + loss_D_B.item())/2
            
    print('Evaluation Loss: [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'%(loss_D/len(val_loader),       # [D loss -]
            loss_G/len(val_loader),       # [G loss -]
            loss_GAN/len(val_loader),     # [adv -]
            loss_cycle/len(val_loader),   # [cycle -]
            loss_identity/len(val_loader),# [identity -]
             ))
    G_AB.train()
    G_BA.train()
    D_A.train()
    D_B.train()
           
def main():
    # data (path)
    # dataset_name = 'gan-getting-started'
    root = 'RGB-T-glass/'
    if not os.path.exists("results"):
        os.makedirs("results")

    # data (img)
    img_height = 256
    img_width = 256
    channels = 3

    # training
    epoch = 0 # epoch to start training from
    n_epochs = 5 # number of epochs of training
    batch_size = 1 # size of the batches
    lr = 0.0002 # adam : learning rate
    b1 = 0.5 # adam : decay of first order momentum of gradient
    b2 = 0.999 # adam : decay of first order momentum of gradient
    decay_epoch = 3 # suggested default : 100 (suggested 'n_epochs' is 200)
                     # epoch from which to start lr decay
                     
    criterion_GAN = torch.nn.MSELoss()
    criterion_cycle = torch.nn.L1Loss()
    criterion_identity = torch.nn.L1Loss()

    input_shape = (channels, img_height, img_width) # (3,256,256)
    n_residual_blocks = 9 # suggested default, number of residual blocks in generator

    G_AB = GeneratorResNet(input_shape, n_residual_blocks)
    G_BA = GeneratorResNet(input_shape, n_residual_blocks)
    D_A = Discriminator(input_shape)
    D_B = Discriminator(input_shape)

    G_AB.apply(weights_init_normal)
    G_BA.apply(weights_init_normal)
    D_A.apply(weights_init_normal)
    D_B.apply(weights_init_normal)

    optimizer_G = torch.optim.Adam(
        itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)
    )

    optimizer_D_A = torch.optim.Adam(
        D_A.parameters(), lr=lr, betas=(b1,b2)
    )
    optimizer_D_B = torch.optim.Adam(
        D_B.parameters(), lr=lr, betas=(b1,b2)
    )

    lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(
        optimizer_G,
        lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step
    )

    lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(
        optimizer_D_A,
        lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step
    )
    lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(
        optimizer_D_B,
        lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step
    )

    transforms_ = [
        transforms.Resize(int(img_height*1.12), InterpolationMode.BICUBIC),
        transforms.RandomCrop((img_height, img_width)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]
     
    transforms_eval = [
        transforms.Resize((int(img_height),int(img_height)), InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]

    cuda = torch.cuda.is_available()
    
    if cuda:
        print("Running in CUDA")
    else:
        print("Running in CPU")
    
    if os.path.exists("checkpoints/model.pt"):
        model = torch.load("checkpoints/model.pt")
        G_AB.load_state_dict(model['generator_state_dict_AB'])
        G_BA.load_state_dict(model['generator_state_dict_BA'])
        D_A.load_state_dict(model['discriminator_state_dict_A'])
        D_B.load_state_dict(model['discriminator_state_dict_B'])
        print("Checkpoint Loaded!!")
                            
    if cuda:
        G_AB = G_AB.cuda()
        G_BA = G_BA.cuda()
        D_A = D_A.cuda()
        D_B = D_B.cuda()
        
        criterion_GAN.cuda()
        criterion_cycle.cuda()
        criterion_identity.cuda()


    n_cpu = 8 # Number of workers
    
    dataloader = DataLoader(
        ImageDataset(root, transforms_=transforms_, unaligned=True),
        batch_size=1, # 1
        shuffle=True,
        num_workers=n_cpu # 3
    )

    val_dataloader = DataLoader(
        ImageDataset(root, transforms_=transforms_eval, unaligned=True, mode='test'),
        batch_size=1,
        shuffle=True,
        num_workers=1
    )
    Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor
    
    print("Training Started!!")
    
    for epoch in range(epoch, n_epochs):
        for i, batch in enumerate(tqdm(dataloader)):
            
            # Set model input
            real_A = batch['A'].type(Tensor)
            real_B = batch['B'].type(Tensor)
            
            # Adversarial ground truths
            valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.
            fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.
            
    # -----------------
    # Train Generators
    # -----------------
            G_AB.train() # train mode
            G_BA.train() # train mode
            
            optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)
            
            # Identity Loss
            loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,
            loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.
                                                                 # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').
            loss_identity = (loss_id_A + loss_id_B)/2
    #         loss_identity = loss_id_A
            
            # GAN Loss
            fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing
            loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'
            fake_A = G_BA(real_B)
            loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'
            
            loss_GAN = (loss_GAN_AB + loss_GAN_BA)/2
            
            # Cycle Loss
            recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo
            loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image
            recov_B = G_AB(fake_A)
            loss_cycle_B = criterion_cycle(recov_B, real_B)
            
            loss_cycle = (loss_cycle_A + loss_cycle_B)/2
            
    # ------> Total Loss
            loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)
            
            loss_G.backward()
            optimizer_G.step()
            
    # -----------------
    # Train Discriminator A
    # -----------------
            optimizer_D_A.zero_grad()
        
            loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real
            loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake
            
            loss_D_A = (loss_real + loss_fake)/2
            
            loss_D_A.backward()
            optimizer_D_A.step()

    # -----------------
    # Train Discriminator B
    # -----------------
            optimizer_D_B.zero_grad()
        
            loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real
            loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake
            
            loss_D_B = (loss_real + loss_fake)/2
            
            loss_D_B.backward()
            optimizer_D_B.step()
            
    # ------> Total Loss
            loss_D = (loss_D_A + loss_D_B)/2
        
    # -----------------
    # Show Progress
    # -----------------
            if (i+1) % 50 == 0:
                test(G_AB, "test", transforms_eval)
                print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'
                        %(epoch+1,n_epochs,       # [Epoch -]
                          i+1,len(dataloader),   # [Batch -]
                          loss_D.item(),       # [D loss -]
                          loss_G.item(),       # [G loss -]
                          loss_GAN.item(),     # [adv -]
                          loss_cycle.item(),   # [cycle -]
                          loss_identity.item(),# [identity -]
                         ))
        PATH = 'checkpoints/model.pt'
        torch.save({
                    'epoch': epoch,
                    'generator_state_dict_AB': G_AB.state_dict(),
                    'generator_state_dict_BA': G_BA.state_dict(),
                    'discriminator_state_dict_A': D_A.state_dict(),
                    'discriminator_state_dict_B': D_B.state_dict(),
                    'optimizer_G_state_dict': optimizer_G.state_dict(),
                    'optimizer_D_A_state_dict': optimizer_D_A.state_dict(),
                    'optimizer_D_B_state_dict': optimizer_D_B.state_dict(),
                    }, PATH)
        evaluate(G_AB, G_BA, D_A, D_B, val_dataloader, criterion_GAN, criterion_cycle, criterion_identity)

if __name__=='__main__':
    main()